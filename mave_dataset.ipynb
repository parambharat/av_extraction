{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext dotenv\n",
    "% dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a subset of the MAVE dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = pd.read_json(\"data/mave_positives.jsonl\", lines=True, orient=\"records\")\n",
    "subset = positive_samples[positive_samples[\"attributes\"].map(len) > 2]\n",
    "category_counts = subset.category.value_counts()\n",
    "subset = subset[subset.category.isin(category_counts.index[:20])]\n",
    "\n",
    "subset = subset.groupby(subset.category).apply(lambda x: x.sample(150, replace=False)).reset_index(drop=True)\n",
    "\n",
    "attribute_keys = subset.attributes.map(lambda x: [item[\"key\"] for item in x])\n",
    "exploded_keys = attribute_keys.explode()\n",
    "exploded_key_counts = exploded_keys.value_counts()\n",
    "subset_keys = exploded_key_counts.index[exploded_key_counts > 50]\n",
    "selected_keys = exploded_keys[(exploded_keys.isin(subset_keys))]\n",
    "keys_index = selected_keys.index.unique()\n",
    "subset = subset.loc[keys_index]\n",
    "subset.category.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">avid-vortex-21</strong>: <a href=\"https://wandb.ai/parambharat/mave/runs/2krgc0r1\" target=\"_blank\">https://wandb.ai/parambharat/mave/runs/2krgc0r1</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221013_131220-2krgc0r1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"mave\", entity=\"parambharat\")\n",
    "raw_dataset = wandb.Artifact(\"raw_dataset\", type=\"dataset\")\n",
    "raw_dataset.add(wandb.Table(dataframe=subset), \"raw_dataset\")\n",
    "wandb.log_artifact(raw_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prepare_dataset(row):\n",
    "    paragraphs = row[\"paragraphs\"]\n",
    "    attributes = row[\"attributes\"]\n",
    "\n",
    "    completion = {}\n",
    "\n",
    "    pids = []\n",
    "    for attribute in attributes:\n",
    "        key = attribute[\"key\"]\n",
    "        for evidence in attribute[\"evidences\"]:\n",
    "            pid = evidence['pid']\n",
    "            source = paragraphs[pid].get('source', pid)\n",
    "            if source in [\"title\", ]:\n",
    "                current = {key: evidence['value']}\n",
    "                if current[key].lower() not in map(lambda x: x.lower(), completion.values()):\n",
    "                    completion[key] = current[key]\n",
    "                    pids.append(pid)\n",
    "    completion[\"category\"] = row[\"category\"]\n",
    "    completion = \" \" + json.dumps(completion) + \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    prompt = \"\"\n",
    "    for pid in set(pids):\n",
    "        source = paragraphs[pid]\n",
    "        prompt += f\"{source.get('text', '')}\\n\"\n",
    "    prompt += \"==>\\n\"\n",
    "\n",
    "    return pd.Series({\"prompt\": prompt, \"completion\": completion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/mave/wandb/run-20221013_132119-2lkakrgq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/parambharat/mave/runs/2lkakrgq\" target=\"_blank\">light-dew-22</a></strong> to <a href=\"https://wandb.ai/parambharat/mave\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000T8A2GU</td>\n",
       "      <td>Cabinet Knobs &amp; Handles</td>\n",
       "      <td>[{'source': 'title', 'text': 'Hickory Hardware...</td>\n",
       "      <td>[{'evidences': [{'begin': 44, 'end': 48, 'pid'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B013VN2YZQ</td>\n",
       "      <td>Cabinet Knobs &amp; Handles</td>\n",
       "      <td>[{'source': 'title', 'text': 'Southern Hills P...</td>\n",
       "      <td>[{'evidences': [{'begin': 15, 'end': 23, 'pid'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B01BOG12PA</td>\n",
       "      <td>Cabinet Knobs &amp; Handles</td>\n",
       "      <td>[{'source': 'title', 'text': 'Cooality KN02SN ...</td>\n",
       "      <td>[{'evidences': [{'begin': 57, 'end': 61, 'pid'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B01549J96M</td>\n",
       "      <td>Cabinet Knobs &amp; Handles</td>\n",
       "      <td>[{'source': 'title', 'text': '(30 Pack) Probri...</td>\n",
       "      <td>[{'evidences': [{'begin': 48, 'end': 54, 'pid'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0088E0JOW</td>\n",
       "      <td>Cabinet Knobs &amp; Handles</td>\n",
       "      <td>[{'source': 'title', 'text': 'Set of 7 Tropica...</td>\n",
       "      <td>[{'evidences': [{'begin': 24, 'end': 31, 'pid'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>B00AGPOR80</td>\n",
       "      <td>Watches</td>\n",
       "      <td>[{'source': 'title', 'text': 'Breitling Men's ...</td>\n",
       "      <td>[{'evidences': [{'begin': 60, 'end': 66, 'pid'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>B00KINCONK</td>\n",
       "      <td>Watches</td>\n",
       "      <td>[{'source': 'title', 'text': 'Tissot Men's T03...</td>\n",
       "      <td>[{'evidences': [{'begin': 222, 'end': 240, 'pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>B00JRVEVHG</td>\n",
       "      <td>Watches</td>\n",
       "      <td>[{'source': 'title', 'text': 'Youyoupifa 5 Pie...</td>\n",
       "      <td>[{'evidences': [{'begin': 0, 'end': 16, 'pid':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>B00NGLO0UG</td>\n",
       "      <td>Watches</td>\n",
       "      <td>[{'source': 'title', 'text': 'Swiss Legend Men...</td>\n",
       "      <td>[{'evidences': [{'begin': 418, 'end': 427, 'pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>B000MLMUO6</td>\n",
       "      <td>Watches</td>\n",
       "      <td>[{'source': 'title', 'text': 'Seiko Men's 5' J...</td>\n",
       "      <td>[{'evidences': [{'begin': 128, 'end': 134, 'pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2998 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                 category  \\\n",
       "0     B000T8A2GU  Cabinet Knobs & Handles   \n",
       "1     B013VN2YZQ  Cabinet Knobs & Handles   \n",
       "2     B01BOG12PA  Cabinet Knobs & Handles   \n",
       "3     B01549J96M  Cabinet Knobs & Handles   \n",
       "4     B0088E0JOW  Cabinet Knobs & Handles   \n",
       "...          ...                      ...   \n",
       "2993  B00AGPOR80                  Watches   \n",
       "2994  B00KINCONK                  Watches   \n",
       "2995  B00JRVEVHG                  Watches   \n",
       "2996  B00NGLO0UG                  Watches   \n",
       "2997  B000MLMUO6                  Watches   \n",
       "\n",
       "                                             paragraphs  \\\n",
       "0     [{'source': 'title', 'text': 'Hickory Hardware...   \n",
       "1     [{'source': 'title', 'text': 'Southern Hills P...   \n",
       "2     [{'source': 'title', 'text': 'Cooality KN02SN ...   \n",
       "3     [{'source': 'title', 'text': '(30 Pack) Probri...   \n",
       "4     [{'source': 'title', 'text': 'Set of 7 Tropica...   \n",
       "...                                                 ...   \n",
       "2993  [{'source': 'title', 'text': 'Breitling Men's ...   \n",
       "2994  [{'source': 'title', 'text': 'Tissot Men's T03...   \n",
       "2995  [{'source': 'title', 'text': 'Youyoupifa 5 Pie...   \n",
       "2996  [{'source': 'title', 'text': 'Swiss Legend Men...   \n",
       "2997  [{'source': 'title', 'text': 'Seiko Men's 5' J...   \n",
       "\n",
       "                                             attributes  \n",
       "0     [{'evidences': [{'begin': 44, 'end': 48, 'pid'...  \n",
       "1     [{'evidences': [{'begin': 15, 'end': 23, 'pid'...  \n",
       "2     [{'evidences': [{'begin': 57, 'end': 61, 'pid'...  \n",
       "3     [{'evidences': [{'begin': 48, 'end': 54, 'pid'...  \n",
       "4     [{'evidences': [{'begin': 24, 'end': 31, 'pid'...  \n",
       "...                                                 ...  \n",
       "2993  [{'evidences': [{'begin': 60, 'end': 66, 'pid'...  \n",
       "2994  [{'evidences': [{'begin': 222, 'end': 240, 'pi...  \n",
       "2995  [{'evidences': [{'begin': 0, 'end': 16, 'pid':...  \n",
       "2996  [{'evidences': [{'begin': 418, 'end': 427, 'pi...  \n",
       "2997  [{'evidences': [{'begin': 128, 'end': 134, 'pi...  \n",
       "\n",
       "[2998 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"mave\", entity=\"parambharat\")\n",
    "artifact = wandb.use_artifact('raw_dataset:latest', type=\"dataset\")\n",
    "subset = artifact.get(\"raw_dataset\")\n",
    "subset = pd.DataFrame(subset.data, columns=subset.columns)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(subset, stratify=subset.category, test_size=0.25)\n",
    "val_df, test_df = train_test_split(test_df, stratify=test_df.category, test_size=0.5)\n",
    "\n",
    "train_df = train_df.apply(prepare_dataset, axis=1)\n",
    "train_df.to_json(\"prompts_dataset_train.jsonl\", lines=True, orient=\"records\")\n",
    "\n",
    "val_df = test_df.apply(prepare_dataset, axis=1)\n",
    "val_df.to_json(\"prompts_dataset_val.jsonl\", lines=True, orient=\"records\")\n",
    "\n",
    "test_df = test_df.apply(prepare_dataset, axis=1)\n",
    "test_df.to_json(\"prompts_dataset_test.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">light-dew-22</strong>: <a href=\"https://wandb.ai/parambharat/mave/runs/2lkakrgq\" target=\"_blank\">https://wandb.ai/parambharat/mave/runs/2lkakrgq</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221013_132119-2lkakrgq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.init(project=\"mave\", entity=\"parambharat\")\n",
    "split_dataset = wandb.Artifact(\"split_dataset\", type=\"dataset\")\n",
    "split_dataset.add(wandb.Table(dataframe=train_df), \"train\")\n",
    "split_dataset.add(wandb.Table(dataframe=val_df), \"val\")\n",
    "split_dataset.add(wandb.Table(dataframe=test_df), \"test\")\n",
    "wandb.log_artifact(split_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"B008VSYQPS\", \"category\": \"Candy & Chocolate\", \"paragraphs\": [{\"source\": \"title\", \"text\": \"Eclipse Spearmint Sugarfree Gum, 18-Piece Pack (3 Packs)\"}, {\"source\": \"description\", \"text\": \"Eclipse Sugar Free Gum, Spearmint\"}, {\"source\": \"description\", \"text\": \"WM. Wrigley Jr. Company, Chicago, IL 60642\"}, {\"source\": \"description\", \"text\": \"MADE OF:SORBITOL, MALTITOL, GUM BASE, GLYCEROL, NATURAL AND ARTIFICIAL FLAVORS, GUM ARABIC; LESS THAN 2% OF: SOY LECITHIN, ASPARTAME, COLOR (TITANIUM DIOXIDE), ACESULFAME K, CARNAUBA WAX, BHT (TO MAINTAIN FRESHNESS).\"}, {\"source\": \"description\", \"text\": \"Remove product from packaging\"}, {\"source\": \"description\", \"text\": \"Statements regarding dietary supplements have not been evaluated by the FDA and are not intended to diagnose, treat, cure, or prevent any disease or health condition.\"}, {\"source\": \"price\", \"text\": \"$2.79\"}, {\"source\": \"brand\", \"text\": \"Eclipse Gum\"}], \"attributes\": [{\"evidences\": [{\"begin\": 8, \"end\": 17, \"pid\": 0, \"value\": \"Spearmint\"}, {\"begin\": 28, \"end\": 31, \"pid\": 0, \"value\": \"Gum\"}, {\"begin\": 19, \"end\": 22, \"pid\": 1, \"value\": \"Gum\"}, {\"begin\": 24, \"end\": 33, \"pid\": 1, \"value\": \"Spearmint\"}, {\"begin\": 29, \"end\": 33, \"pid\": 1, \"value\": \"mint\"}, {\"begin\": 28, \"end\": 31, \"pid\": 3, \"value\": \"GUM\"}, {\"begin\": 80, \"end\": 83, \"pid\": 3, \"value\": \"GUM\"}, {\"begin\": 8, \"end\": 11, \"pid\": 7, \"value\": \"Gum\"}], \"key\": \"Type\"}, {\"evidences\": [{\"begin\": 18, \"end\": 27, \"pid\": 0, \"value\": \"Sugarfree\"}, {\"begin\": 8, \"end\": 18, \"pid\": 1, \"value\": \"Sugar Free\"}], \"key\": \"Sugar Content\"}, {\"evidences\": [{\"begin\": 28, \"end\": 31, \"pid\": 0, \"value\": \"Gum\"}, {\"begin\": 19, \"end\": 22, \"pid\": 1, \"value\": \"Gum\"}, {\"begin\": 28, \"end\": 31, \"pid\": 3, \"value\": \"GUM\"}, {\"begin\": 80, \"end\": 83, \"pid\": 3, \"value\": \"GUM\"}, {\"begin\": 8, \"end\": 11, \"pid\": 7, \"value\": \"Gum\"}], \"key\": \"Candy Variety\"}]}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(json.loads(subset.sample(1).to_json(lines=True, orient=\"records\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai tools fine_tunes.prepare_data -f prompts_dataset_train.jsonl -q\n",
    "!openai tools fine_tunes.prepare_data -f prompts_dataset_val.jsonl -q\n",
    "!openai tools fine_tunes.prepare_data -f prompts_dataset_test.jsonl -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"prompts_dataset_train_prepared.jsonl\", lines=True, orient=\"records\")\n",
    "val_df = pd.read_json(\"prompts_dataset_val_prepared.jsonl\", lines=True, orient=\"records\")\n",
    "test_df = pd.read_json(\"prompts_dataset_test_prepared.jsonl\", lines=True, orient=\"records\")\n",
    "\n",
    "wandb.init(project=\"mave\", entity=\"parambharat\")\n",
    "prepared_dataset = wandb.Artifact(\"prepared_dataset\", type=\"dataset\")\n",
    "prepared_dataset.add(wandb.Table(dataframe=train_df), \"train\")\n",
    "prepared_dataset.add(wandb.Table(dataframe=val_df), \"val\")\n",
    "prepared_dataset.add(wandb.Table(dataframe=test_df), \"test\")\n",
    "wandb.log_artifact(prepared_dataset)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Finetuning GPT 3 model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export $(cat ./.env | grep -v ^# | xargs) >/dev/null\n",
    "# !openai api fine_tunes.create -t \"prompts_dataset_train_prepared.jsonl\" -v \"prompts_dataset_val_prepared.jsonl\" -m ada --suffix \"mave attribute recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_items = train_df[\"completion\"].str.strip(\"\\n\\n###\\n\\n\").map(json.loads)\n",
    "shirts = loaded_items[loaded_items.map(lambda x: x[\"category\"] == \"Shirts & Tops\")]\n",
    "\n",
    "a = shirts.iloc[5]\n",
    "b = shirts.iloc[4]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mparambharat\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/mave/wandb/run-20221017_202206-27gu1k7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/parambharat/mave/runs/27gu1k7u\" target=\"_blank\">sweet-sponge-28</a></strong> to <a href=\"https://wandb.ai/parambharat/mave\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"mave\", entity=\"parambharat\", job_type=\"eval\", reinit=False)\n",
    "\n",
    "finetune_artifact = run.use_artifact('parambharat/mave/fine_tune_details:v26', type='fine_tune_details')\n",
    "finetune_dir = finetune_artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update({k: finetune_artifact.metadata[k] for k in ['fine_tuned_model', 'model', 'hyperparams']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m:   3 of 3 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "validation_artifact = run.use_artifact(f'parambharat/mave/prepared_dataset:latest', type=\"dataset\")\n",
    "val_table = validation_artifact.get(\"val\")\n",
    "val_df = pd.DataFrame(val_table.data, columns=val_table.columns)\n",
    "test_table = validation_artifact.get(\"test\")\n",
    "test_df = pd.DataFrame(test_table.data, columns=test_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "config.fine_tuned_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the finetuned model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_completions(val_df):\n",
    "    data = []\n",
    "\n",
    "    for _, row in tqdm.tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "        prompt = row['prompt']\n",
    "        res = openai.Completion.create(model=config.fine_tuned_model, prompt=prompt, max_tokens=256,\n",
    "                                       stop=[\"\\n\\n###\\n\\n\"])\n",
    "        completion = res['choices'][0]['text']\n",
    "        prompt = prompt[:-5]  # remove \"\\n==>\\n\"\n",
    "        target = row['completion'][1:-7]  # remove initial space and \"END\"\n",
    "        data.append([prompt, target, completion])\n",
    "    return pd.DataFrame(data, columns=[\"prompt\", \"reference\", \"prediction\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dict_similar(row):\n",
    "    reference = row[\"reference\"]\n",
    "    prediction = row[\"prediction\"]\n",
    "    try:\n",
    "        reference = json.loads(reference)\n",
    "        prediction = json.loads(prediction)\n",
    "        common = len(set(reference.items()) & set(prediction.items()))\n",
    "        actual = len(reference.items())\n",
    "        return common / actual\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def prompt_to_bio(row, label_key=\"target\"):\n",
    "    prompt = row[\"prompt\"]\n",
    "    target = row[label_key]\n",
    "    try:\n",
    "        target = json.loads(target)\n",
    "        prompt = prompt.split()\n",
    "        labels = [\"O\"] * len(prompt)\n",
    "    except:\n",
    "        labels = [\"O\"]\n",
    "        return labels\n",
    "\n",
    "    for attribute, value in target.items():\n",
    "        values = value.split()\n",
    "        start_ent = False\n",
    "        for idx, word in enumerate(values):\n",
    "            try:\n",
    "                first_idx = prompt.index(word)\n",
    "                if idx == 0:\n",
    "                    first_idx = prompt.index(word)\n",
    "                    labels[first_idx] = f\"B-{attribute}\"\n",
    "                    start_ent = True\n",
    "                elif start_ent:\n",
    "                    first_idx = prompt.index(word)\n",
    "                    labels[first_idx] = f\"I-{attribute}\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return labels\n",
    "\n",
    "\n",
    "def to_category(row):\n",
    "    reference = json.loads(row[\"reference\"])[\"category\"]\n",
    "    try:\n",
    "        prediction = json.loads(row[\"prediction\"])[\"category\"]\n",
    "    except Exception:\n",
    "        return pd.Series({\"reference_category\": reference, \"predicted_category\": \"\"})\n",
    "    return pd.Series({\"reference_category\": reference, \"predicted_category\": prediction})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 369/369 [03:19<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "results_df = predict_completions(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_labels = partial(prompt_to_bio, label_key=\"reference\")\n",
    "prompt_to_predictions = partial(prompt_to_bio, label_key=\"prediction\")\n",
    "results_df[\"reference_labels\"] = results_df.apply(prompt_to_labels, axis=1)\n",
    "results_df[\"predicted_labels\"] = results_df.apply(prompt_to_predictions, axis=1)\n",
    "results_df[[\"reference_category\", \"predicted_category\"]] = results_df.apply(to_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def evaluate_results(results_df):\n",
    "    results_df = results_df[results_df.reference_labels.map(len) == results_df.predicted_labels.map(len)]\n",
    "    results_df[\"exact_match_score\"] = results_df.apply(score_dict_similar, axis=1)\n",
    "    seq_results = metric.compute(\n",
    "        predictions=results_df[\"predicted_labels\"].tolist(),\n",
    "        references=results_df[\"reference_labels\"].tolist())\n",
    "\n",
    "    seq_results = (pd.DataFrame(seq_results)\n",
    "                   .T\n",
    "                   .reset_index()\n",
    "                   .rename({\"index\": \"label\"}, axis=1)\n",
    "                   )\n",
    "\n",
    "    clf_results = clf_report = classification_report(\n",
    "        y_true=results_df[\"reference_category\"],\n",
    "        y_pred=results_df[\"predicted_category\"],\n",
    "        output_dict=True)\n",
    "    clf_results = (pd.DataFrame(clf_results)\n",
    "                   .T\n",
    "                   .reset_index()\n",
    "                   .rename({\"index\": \"label\"}, axis=1)\n",
    "                   )\n",
    "    return results_df, seq_results, clf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mugan/anaconda3/envs/mave/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mugan/anaconda3/envs/mave/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mugan/anaconda3/envs/mave/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mugan/anaconda3/envs/mave/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mugan/anaconda3/envs/mave/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results_df, seq_results, clf_results = evaluate_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"test_predictions\": wandb.Table(dataframe=results_df),\n",
    "           \"test_seq_eval_metrics\": wandb.Table(dataframe=seq_results),\n",
    "           \"test_classification_metrics\": wandb.Table(dataframe=clf_results),\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.run.summary[\"test_exact_match_score\"] = results_df.exact_match_score.describe().to_dict()\n",
    "# wandb.run.summary[\"test_classification_metric\"] = json.loads(clf_results.loc[22:].set_index(\"label\").T.to_json())\n",
    "wandb.run.summary[\"test_seqeval_metrics\"] = json.loads(\n",
    "    seq_results[(seq_results\n",
    "                 .label\n",
    "                 .str\n",
    "                 .startswith(\"overall\"))]\n",
    "    .set_index(\"label\")\n",
    "    .T\n",
    "    .to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7816091954022988"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_results[seq_results.label == \"overall_f1\"][\"f1\"].values[0]\n",
    "clf_results[clf_results.label == \"macro avg\"][\"f1-score\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"exact_match_score\"] = results_df.exact_match_score.mean()\n",
    "wandb.run.summary[\"seqeval_f1score\"] = seq_results[seq_results.label == \"overall_f1\"][\"f1\"].values[0]\n",
    "wandb.run.summary[\"classification_f1score\"] = clf_results[clf_results.label == \"macro avg\"][\"f1-score\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f8e29728dc4643adc022698c140b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.427 MB of 0.443 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.964135…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_f1score</td><td>0.89387</td></tr><tr><td>exact_match_score</td><td>0.81206</td></tr><tr><td>seqeval_f1score</td><td>0.78161</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sweet-sponge-28</strong>: <a href=\"https://wandb.ai/parambharat/mave/runs/27gu1k7u\" target=\"_blank\">https://wandb.ai/parambharat/mave/runs/27gu1k7u</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221017_202206-27gu1k7u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
